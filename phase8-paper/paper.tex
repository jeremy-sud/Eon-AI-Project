% Proyecto Eón - Paper Académico
% Template LaTeX para arXiv / TinyML Summit
%
% (c) 2024 Sistemas Ursol - Jeremy Arias Solano

\documentclass[10pt,twocolumn]{article}

\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Eón: Inteligencia Emergente con Recursos Mínimos}

\author{
  Jeremy Arias Solano \\
  Sistemas Ursol \\
  \texttt{github.com/jeremy-sud}
}

\date{Diciembre 2024}

\begin{document}

\maketitle

\begin{abstract}
Presentamos \textbf{Eón}, una arquitectura de inteligencia artificial basada en
Reservoir Computing que demuestra la emergencia de comportamiento inteligente
con recursos extraordinariamente mínimos ($\sim$1KB de memoria). A diferencia de
los modelos de lenguaje modernos que requieren gigabytes de memoria y billones
de operaciones, Eón logra predicción de series temporales caóticas con menos de
1,500 bytes de huella de memoria. Implementamos Eón en tres plataformas (Python,
C, JavaScript) y demostramos su viabilidad en microcontroladores de \$3.
\end{abstract}

\section{Introducción}

La industria de IA actual está dominada por la escala. GPT-4 tiene estimados
1.7 trillones de parámetros, mientras que incluso modelos ``pequeños'' como
BERT Base usan 110 millones de parámetros ($\sim$432MB en FP32).

Este trabajo explora la dirección opuesta: \textbf{¿cuán pequeño puede ser
un sistema que exhiba comportamiento inteligente?}

\subsection{Contribuciones}

\begin{enumerate}
  \item Implementación de ESN en punto fijo Q8.8 con 1.3KB
  \item Cuantización hasta 1-bit con retención de 80\% de precisión
  \item Plasticidad Hebbiana para aprendizaje continuo
  \item Código abierto en Python, C y JavaScript
\end{enumerate}

\section{Antecedentes}

\subsection{Echo State Networks}

Los ESN~\cite{jaeger2001} son un tipo de Reservoir Computing donde una red
recurrente aleatoria (reservoir) actúa como proyector de alta dimensión,
entrenando únicamente una capa de salida lineal.

\begin{equation}
  \mathbf{x}(t) = \tanh(W_{in}\mathbf{u}(t) + W\mathbf{x}(t-1))
\end{equation}

\begin{equation}
  \mathbf{y}(t) = W_{out}\mathbf{x}(t)
\end{equation}

Solo $W_{out}$ se entrena, típicamente por regresión lineal.

\subsection{TinyML}

TinyML se refiere a la ejecución de modelos de machine learning en
microcontroladores con severas restricciones de memoria (típicamente
$<$256KB RAM) y poder de cómputo.

\section{Arquitectura de Eón}

\subsection{Punto Fijo Q8.8}

Para compatibilidad con MCUs sin FPU, usamos aritmética de punto fijo:

\begin{itemize}
  \item Pesos: \texttt{int16\_t} con 8 bits enteros y 8 decimales
  \item Rango: $[-128, 127.996]$
  \item Operaciones: shift en lugar de división ($\gg 8$)
\end{itemize}

\subsection{Reservoir Escaso}

Reducimos conexiones manteniendo solo $\frac{1}{4}$ del reservoir denso:

\begin{equation}
  \text{Memoria} = N \cdot \frac{N}{4} \cdot 2 + N \cdot 2 + N \cdot 2
\end{equation}

Para $N=32$: $512 + 64 + 64 = 640$ bytes solo para pesos.

\section{Experimentos}

\subsection{Predicción Mackey-Glass}

Serie temporal caótica estándar para evaluación:

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Implementación & MSE & Memoria \\
\midrule
Python (float64) & 0.0004 & 80 KB \\
C (Q8.8) & 0.009 & 1.3 KB \\
JavaScript & 0.010 & $\sim$4 KB \\
\bottomrule
\end{tabular}
\caption{Resultados en Mackey-Glass}
\end{table}

\subsection{Comparación con Estado del Arte}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Modelo & Memoria & Factor \\
\midrule
GPT-2 Small & 500 MB & 384,615$\times$ \\
BERT Tiny & 16 MB & 12,307$\times$ \\
TinyBERT & 60 MB & 46,153$\times$ \\
\textbf{Eón} & \textbf{1.3 KB} & \textbf{1$\times$} \\
\bottomrule
\end{tabular}
\caption{Comparación de huella de memoria}
\end{table}

\section{Discusión}

Eón no pretende competir con LLMs en tareas de lenguaje natural.
Su valor está en demostrar que:

\begin{enumerate}
  \item La inteligencia puede emerger de recursos mínimos
  \item El reservoir aleatorio contiene computación latente
  \item La cuantización agresiva preserva información esencial
\end{enumerate}

\section{Conclusiones}

Presentamos Eón, un sistema de IA que opera en 1.3KB de memoria.
El código está disponible en:

\url{https://github.com/SistemasUrsol/eon-project}

\section*{Agradecimientos}

A la comunidad de Reservoir Computing y TinyML.

\begin{thebibliography}{9}

\bibitem{jaeger2001}
Jaeger, H. (2001).
The ``echo state'' approach to analysing and training recurrent neural networks.
GMD Report 148.

\bibitem{bi1998}
Bi, G. \& Poo, M. (1998).
Synaptic modifications in cultured hippocampal neurons.
Journal of Neuroscience.

\bibitem{devlin2019}
Devlin, J. et al. (2019).
BERT: Pre-training of Deep Bidirectional Transformers.
NAACL-HLT.

\end{thebibliography}

\end{document}
